1. 多头自注意力层（Multi-Headed Self-Attention, MSA）

自注意力机制（Self-Attention） 是Transformer的核心组件，旨在让模型根据输入序列中的每个元素与其他元素的相关性分配不同的权重。这种机制让模型可以捕捉输入序列中任意两个位置之间的全局依赖关系，而不仅仅局限于局部信息。

* 单头自注意力：在自注意力中，每个输入向量（比如一个词或图像块的表示）通过查询（Query）、键（Key）、和值（Value）三个向量的内积计算与其他输入向量的相关性，并根据相关性重新加权输出结果。这样，模型可以对重要的输入部分分配更多注意力。
* 多头自注意力：多头注意力是指模型并行地执行多次独立的自注意力计算（即有多个头，每个头计算一次自注意力），然后将这些结果拼接在一起。每个头在不同的子空间中学习不同的相关性，这让模型能够从多个角度捕捉输入序列的特征，增强模型的表达能力。

作用：多头自注意力层使模型能够同时关注输入中的多个位置和特征，提升捕捉复杂特征关系的能力。

2. 前馈神经网络层（MLP）

前馈神经网络（Multilayer Perceptron, MLP） 是一种经典的神经网络层，由若干个全连接层组成。它的工作原理是将输入进行非线性变换，从而增加模型的非线性表达能力。

* 在Transformer中，前馈神经网络层通常由两层全连接层（FC）和一个激活函数（如GELU或ReLU）组成。每个输入向量独立地通过前馈网络进行处理。

作用：前馈神经网络层用来对每个输入向量进行进一步的非线性变换，使得模型能够更好地拟合复杂的模式和特征。

3. Layernorm（层归一化）

层归一化（Layer Normalization, Layernorm） 是一种归一化技术，用来提高深层神经网络的训练稳定性。

* Layernorm的作用是对输入在层的维度上进行标准化，即对每个输入向量的所有特征进行归一化。它通过对输入的均值和标准差进行计算，将其调整到零均值、单位方差的分布，这样可以使梯度传播更为稳定。

与批归一化（BatchNorm）不同，Layernorm是针对每个样本独立应用的，这使得它在处理序列数据（如Transformer）时更加适用。

作用：Layernorm通过归一化每一层的输出，帮助稳定训练，避免梯度消失或梯度爆炸问题。

4. 残差连接（Residual Connection）

残差连接，也叫跳跃连接（skip connection），是一种将某一层的输入直接传递到后续层输出的技术。它的公式是：

$$
y = F(x) + x
$$

其中，F(x) 是层的非线性变换结果，x 是输入。残差连接通过引入输入的快捷路径，绕过了中间的变换层，避免了深度网络中的梯度消失问题。

作用：残差连接让网络更易于训练，尤其是深层网络。它还帮助网络保持输入的基本信息，避免层数增加导致模型性能下降（即网络退化问题）。
