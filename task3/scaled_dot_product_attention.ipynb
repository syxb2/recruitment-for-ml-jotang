{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baijiale/Documents/Code/recruitment_for_ml_jotang/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenizer和输入处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2051, 10029,  2066,  2019,  8612]])\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "text = \"time flies like an arrow\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "print(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokenizer: AutoTokenizer基于BERT的bert-base-uncased模型，将输入文本分词并转化为ID向量。return_tensors=\"pt\"指定返回的是PyTorch张量。\n",
    "* 输入输出: 将文本 \"time flies like an arrow\" 转换成分词后的ID（词汇表中的整数索引）。输出类似tensor([[101, 2078, ...]])。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 嵌入层（Embedding Layer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(30522, 768)\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "print(token_emb)\n",
    "\n",
    "inputs_embeds = token_emb(inputs.input_ids)\n",
    "print(inputs_embeds.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AutoConfig: AutoConfig从预训练模型中加载配置，获取vocab_size（词汇表大小）和hidden_size（隐藏层维度）。\n",
    "* Embedding: nn.Embedding是PyTorch中的嵌入层，它把token ID映射到嵌入向量（大小为hidden_size）。\n",
    "* inputs_embeds: 将输入的token ID转化为嵌入表示，形状为(batch_size, seq_len, hidden_size)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这段代码中，batch_size、seq_len 和 hidden_size 分别代表如下含义：\n",
    "\n",
    "1.\tbatch_size:\n",
    "\n",
    "* 含义: 批量大小，即同时处理的样本数量。\n",
    "* 在自然语言处理中，通常我们会将多个句子组成一个批次，传入模型一次处理。\n",
    "* 在你的代码中，inputs来自一个句子 \"time flies like an arrow\"，所以batch_size = 1（因为这里只有一个句子）。\n",
    "\n",
    "2.\tseq_len:\n",
    "\n",
    "* 含义: 序列长度，即输入文本的token数量（分词后的长度）。\n",
    "* 在你的示例中，输入句子 \"time flies like an arrow\"被分词为 [time, flies, like, an, arrow]，所以seq_len = 5。\n",
    "* 如果是更长的句子或有多个句子，则seq_len会根据输入的不同而变化。\n",
    "\n",
    "3.\thidden_size:\n",
    "\n",
    "* 含义: 隐藏层的维度大小（即每个token的嵌入向量的维度）。BERT的hidden_size在bert-base-uncased模型中是 768。\n",
    "* 这意味着每个输入token都被表示为一个768维的向量。\n",
    "\n",
    "在代码中：\n",
    "\n",
    "* inputs_embeds 的形状是 (batch_size, seq_len, hidden_size)，即 (1, 5, 768)。\n",
    "* 1 表示有一个句子（batch_size）。\n",
    "* 5 表示该句子中有5个token（seq_len）。\n",
    "* 768 表示每个token用768维的向量表示（hidden_size）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 自注意力计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[28.0974,  0.9781,  0.5916,  1.2912, -1.0577],\n",
      "         [ 0.9781, 27.6146,  0.4287, -1.6370,  0.1412],\n",
      "         [ 0.5916,  0.4287, 28.5915, -2.2223,  0.8335],\n",
      "         [ 1.2912, -1.6370, -2.2223, 26.7196, -2.0823],\n",
      "         [-1.0577,  0.1412,  0.8335, -2.0823, 24.6930]]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Q = K = V = inputs_embeds\n",
    "dim_k = K.size(-1)\n",
    "scores = torch.bmm(Q, K.transpose(1,2)) / sqrt(dim_k)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q, K, V: 在简单的自注意力机制中，Query（Q）、Key（K）和Value（V）通常是同一个输入（inputs_embeds）。\n",
    "* dim_k: 代表Key的维度大小（通常与隐藏层维度一致），用于缩放点积注意力中的分母，防止数值过大。\n",
    "* scores: 通过torch.bmm（批量矩阵乘法）计算Q和K的点积（*即将矩阵乘法对应位置的两个词向量（嵌入向量）做点乘*），再除以sqrt(dim_k)，得到注意力得分。scores的大小为(batch_size, seq_len, seq_len)，表示每个单词对其他单词的相关性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在自注意力机制（Self-Attention）中，Query（Q）、Key（K）和Value（V）是输入数据的三种不同的表示，它们在计算注意力权重时扮演着不同的角色。具体来说，Q、K、V分别用来确定句子中每个词对于其他词的相关性和权重，最终生成注意力输出。以下是它们的详细解释：\n",
    "\n",
    "1. Query（Q）\n",
    "\n",
    "* 含义: Query可以看作是需要“查询”的词，它决定了我们要关注哪一个词的语义信息。\n",
    "* 功能: Query与其他词的Key进行点积运算，得到当前词对其他词的相关性评分。简而言之，Query告诉我们”我要查询哪些信息”。\n",
    "* 具体作用: 在自注意力机制中，对于每个词来说，它自己的嵌入向量会被映射成Query向量，用来与其他词的Key进行匹配。\n",
    "\n",
    "2. Key（K）\n",
    "\n",
    "* 含义: Key是每个词的表示，它和Query配对，用来决定该词在其他词的注意力分数中应占的比重。\n",
    "* 功能: Query和Key的点积会告诉我们，这个Query和Key是否“相关”，或者说它们之间的语义距离有多近。\n",
    "* 具体作用: Key可以理解为“标识符”，它表示该词的某种特征，允许其他词的Query和它进行比较，决定注意力分配。\n",
    "\n",
    "3. Value（V）\n",
    "\n",
    "* 含义: Value是每个词的表示，它存储了该词携带的所有信息。注意力权重将应用到Value上，生成新的表示。\n",
    "* 功能: 在得到了Query和Key计算出的注意力权重后，Value会加权求和，最终输出该词对当前词的贡献信息。\n",
    "* 具体作用: Value可以看作是携带实际信息的容器，经过加权后的Value向量将是自注意力层的输出。\n",
    "\n",
    "在这段代码中，直接使用相同的嵌入向量来充当Query、Key和Value。在实际的自注意力机制中，Q、K、V通常是通过不同的线性变换矩阵生成的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*可以看到，点积 scores 为一个 `(1 *) 5 * 5` 的对称矩阵，对角线上的项的值明显大得多，因为 token 与它自己是最相似的*\n",
    "\n",
    "*如位置 ij 表示，第 i 个 token 与 第 j 个 token 的相似程度*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Softmax和权重计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "weights = F.softmax(scores, dim=-1)\n",
    "print(weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Softmax: 对注意力得分进行Softmax归一化，得到权重，表示每个单词对其他单词的注意力分布。\n",
    "* weights.sum(dim=-1): 验证Softmax的归一化性质，即每一行的权重和应为1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 注意力输出计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "attn_outputs = torch.bmm(weights, V)\n",
    "print(attn_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attn_outputs: 使用注意力权重weights和Value V做加权求和，得到最终的注意力输出，大小为(batch_size, seq_len, hidden_size)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
