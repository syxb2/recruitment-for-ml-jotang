# task0

为了了解机器学习的基础知识，我选择听了 CS50 的课程：Introduction to AI with Python，再配合学习学长们推荐的河北工大刘老师的 pytorch 实践课程。故此 markdown 文件并不是主要，主要是 pdf 文件中我的笔记以及自己的理解。

我的笔记基本都在课程讲义或者课程powerpoint中。这样结构可以更清晰，也方便查找。

## 题目问题

### 监督学习与无监督学习的区别

* 监督学习中，每个样本都有一个标签，模型的目标是预测这个标签。
* 无监督学习中，样本没有标签，目标是找到数据中的一些规律化结构。

> 在 `cs50ai_lecture_4.pdf` 中有我做的笔记。

### 机器学习和深度学习的区别

机器学习是人工智能中一个大的分支，通过数据来训练模型，然后用其来预测新的数据。而深度学习是机器学习下的一个细分，其使用神经网络来训练模型。（深度指的是模型网络中有多层）

### 偏导数、链式法则、梯度、矩阵等数学概念在机器学习中的作用

1. 偏导数：在梯度下降算法中，求解损失函数的梯度就需要用到偏导数。
2. 链式法则：在反向传播算法中，需要用到链式法则，从 loss 开始，求解每一层 linear 的梯度，最后相乘，结果就是我们需要的 loss 关于 w 向量的梯度。这个过程就是根据链式法则的原理而来。*详见 `hebut_lecture_04_back_propagation.pdf` 第 8 页*。
3. 梯度：梯度下降算法就是通过负的梯度乘以学习率来更新权重，使得损失函数最小化。
4. 矩阵：在神经网络中，权重，输入，输出等都是以矩阵（向量）的形式表示的。

### 常见的激活函数

1. 阶跃函数

$$
f(x) = \begin{cases}
1, & x > 0 \\
0, & x \leq 0
\end{cases}
$$

2. Sigmoid 函数：

$$
f(x) = \frac{e^x}{e^{x} + 1}
$$

或

$$
f(x) = \frac{1}{e^{-x} + 1}
$$


3. Tanh 函数：

$$
f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

4. ReLU 函数：

$$
f(x) = max(0, x)
$$

详见 `cs50ai_lecture_5.pdf` 第 1 页。

### 神经网络的基本结构

神经网络包含输入层、隐藏层、输出层、损失函数等。其中：

* 隐藏层可以有多层。每一层有多个神经元。每个神经元有激活函数和权重。
* 输入层的神经元数目等于输入的特征数目。
* 每个神经网络只有一个损失函数，用于计算模型的误差。

详见 `cs50ai_lecture_5.pdf` 第 6 页。

### 机器学习中的数据处理

机器学习中的数据处理有很多种，这里列举我熟悉的几种。

1. 在图像识别中，可以对图像进行预处理（增强），对图像进行缩放、裁剪、旋转、翻转、归一化等。
2. 数据清理，很多数据存在缺失值，我们可以使用均值，中位数等方法来填充缺失值，也可以将含有缺失值的数据直接删除
3. 数据变换，如将数据缩放到一个特定的范围（归一化）。常见的数据变换方法还有辨准化、对数变换等

## 一些自己的理解

### 1. 对输出层梯度公式、隐藏层梯度公式的理解

![](<media/截屏2024-10-08 下午8.12.40.png>)

### 2. 对反向传播算法的理解

![](<media/截屏2024-10-08 下午8.14.42.png>)

### 3. 对梯度下降、随机梯度下降、混合型梯度下降的理解

![](<media/截屏2024-10-08 下午8.16.00.png>)

<!-- ps：请忽略我的烂字 -->

### 4. 对激活函数的理解

![](<media/截屏2024-10-08 下午8.18.06.png>)

激活函数也就是节点中要进行的计算（将输入信号转换为输出信号）。类似人神经元中电信号输入输出的转换。

激活函数也用于引入非线性。

### 5. 对 logistic 激活函数的理解（回归和分类的关系）

![](<media/截屏2024-10-12 下午8.52.01.png>)

### 6. 为什么要引入激活函数？

![](<media/截屏2024-10-12 下午8.53.32.png>)

### 7. softmax 函数的理解

是一个可以做到：输出多个值、每个值都在 0-1 之间、所有输出值的和为 1。

对多分类损失函数的理解：

![](<media/截屏2024-10-12 下午9.01.28.png>)
